{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import time\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    average_precision_score, precision_recall_curve, roc_curve,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/drive/My Drive/dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "holfrWlZBtbX",
        "outputId": "cc3b268f-ec9d-4eb5-84f9-149a7d458d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Shape: (31498, 688)\n",
            "\n",
            "First 5 rows:\n",
            "         id target sample_type  continuous_0  categorical_0  categorical_1  \\\n",
            "0  14537510   good         dev           1.0            0.0            0.0   \n",
            "1  12527950   good         dev           1.0            1.0            1.0   \n",
            "2  12277478   good         dev           1.0            1.0            1.0   \n",
            "3  13362203   good         dev           1.0            1.0            2.0   \n",
            "4  14036057    bad         dev           1.0            1.0            1.0   \n",
            "\n",
            "   categorical_2  categorical_3  categorical_4  categorical_5  ...  \\\n",
            "0            0.0            0.0            0.0            0.0  ...   \n",
            "1            1.0            1.0            1.0            0.0  ...   \n",
            "2            1.0            1.0            2.0            0.0  ...   \n",
            "3            2.0            2.0            3.0            0.0  ...   \n",
            "4            1.0            1.0            1.0            0.0  ...   \n",
            "\n",
            "   continuous_660  continuous_661  continuous_662  continuous_663  \\\n",
            "0      315.000000          315.00          315.00             0.0   \n",
            "1      155.000000          155.00          155.00             0.0   \n",
            "2      163.125000          162.15          164.10             0.0   \n",
            "3      107.226666           75.00          153.68             0.0   \n",
            "4      157.150000          155.30          158.45           138.0   \n",
            "\n",
            "   continuous_664  continuous_665  continuous_666  categorical_15  \\\n",
            "0             0.0             0.0             0.0             0.0   \n",
            "1             0.0             0.0             0.0             0.0   \n",
            "2             0.0             0.0             0.0             1.0   \n",
            "3             0.0             0.0             0.0             2.0   \n",
            "4            46.0             0.0           120.0             3.0   \n",
            "\n",
            "   categorical_16  continuous_667  \n",
            "0             0.0             NaN  \n",
            "1             1.0             0.0  \n",
            "2             1.0             NaN  \n",
            "3             2.0             0.0  \n",
            "4             1.0             NaN  \n",
            "\n",
            "[5 rows x 688 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General preliminary view of the dataset\n",
        "print(\"Dataset statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Identify categorical and continuous columns\n",
        "categorical_cols = [col for col in df.columns if 'categorical' in col.lower()]\n",
        "continuous_cols = [col for col in df.columns if 'continuous' in col.lower()]\n",
        "total_cols_count = len(df.columns)\n",
        "\n",
        "print(f\"There are {len(categorical_cols)} categorical columns and {len(continuous_cols)} continuous columns among {total_cols_count} columns\")\n",
        "\n",
        "# Split the dataset based on 'sample_type' column\n",
        "df_dev = df[df['sample_type'] == 'dev'].copy()\n",
        "df_test = df[df['sample_type'] == 'test'].copy()\n",
        "df_valid = df[df['sample_type'] == 'valid'].copy()\n",
        "\n",
        "print(f\"\\nDataset split sizes: Dev={len(df_dev)}, Test={len(df_test)}, Valid={len(df_valid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4djqDTAS89w",
        "outputId": "d4354be5-1371-466e-b7df-c751f42637eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset statistics:\n",
            "                 id  continuous_0  categorical_0  categorical_1  \\\n",
            "count  3.149800e+04  31498.000000   31498.000000   31498.000000   \n",
            "mean   1.254142e+07      0.983777       1.352022       8.705188   \n",
            "std    2.301706e+06      0.126335       1.002153       6.513199   \n",
            "min    9.004705e+06      0.000000       0.000000       0.000000   \n",
            "25%    1.035565e+07      1.000000       1.000000       3.000000   \n",
            "50%    1.230090e+07      1.000000       1.000000       8.000000   \n",
            "75%    1.447697e+07      1.000000       1.000000      13.000000   \n",
            "max    1.712175e+07      1.000000       7.000000      26.000000   \n",
            "\n",
            "       categorical_2  categorical_3  categorical_4  categorical_5  \\\n",
            "count   31498.000000   31498.000000   31498.000000   31498.000000   \n",
            "mean        8.717823       8.729126       8.936980       0.512890   \n",
            "std         6.527480       6.529695       6.759147       0.982387   \n",
            "min         0.000000       0.000000       0.000000       0.000000   \n",
            "25%         3.000000       3.000000       3.000000       0.000000   \n",
            "50%         8.000000       8.000000       8.000000       0.000000   \n",
            "75%        13.000000      13.000000      13.000000       0.000000   \n",
            "max        26.000000      26.000000      26.000000       5.000000   \n",
            "\n",
            "       categorical_6  categorical_7  ...  continuous_660  continuous_661  \\\n",
            "count   31498.000000   31498.000000  ...    16405.000000    16405.000000   \n",
            "mean        0.461966       0.535812  ...      107.082979       86.617275   \n",
            "std         0.931930       1.021099  ...       87.298676       89.529727   \n",
            "min         0.000000       0.000000  ...        0.000000        0.000000   \n",
            "25%         0.000000       0.000000  ...       55.166666       18.000000   \n",
            "50%         0.000000       0.000000  ...       90.000000       75.000000   \n",
            "75%         0.000000       1.000000  ...      150.000000      125.000000   \n",
            "max         5.000000       5.000000  ...     1510.000000     1510.000000   \n",
            "\n",
            "       continuous_662  continuous_663  continuous_664  continuous_665  \\\n",
            "count    16405.000000    16405.000000    16405.000000    16405.000000   \n",
            "mean       128.735768       43.940013       15.567802        4.295324   \n",
            "std         97.377377      147.545091       51.530766       26.464109   \n",
            "min          0.000000        0.000000        0.000000        0.000000   \n",
            "25%         75.000000        0.000000        0.000000        0.000000   \n",
            "50%        106.000000        0.000000        0.000000        0.000000   \n",
            "75%        162.000000       20.000000        6.666666        0.000000   \n",
            "max       1510.000000     9030.000000     3010.000000     1200.000000   \n",
            "\n",
            "       continuous_666  categorical_15  categorical_16  continuous_667  \n",
            "count    16405.000000    21025.000000    16477.000000    23746.000000  \n",
            "mean        30.841700       11.140832        7.656430        0.024846  \n",
            "std         91.330445       23.194343        5.937698        0.155660  \n",
            "min          0.000000        0.000000        0.000000        0.000000  \n",
            "25%          0.000000        3.000000        2.000000        0.000000  \n",
            "50%          0.000000        4.000000        6.000000        0.000000  \n",
            "75%         18.000000       10.000000       13.000000        0.000000  \n",
            "max       5220.000000      240.000000       23.000000        1.000000  \n",
            "\n",
            "[8 rows x 686 columns]\n",
            "There are 17 categorical columns and 668 continuous columns among 688 columns\n",
            "\n",
            "Dataset split sizes: Dev=19368, Test=6130, Valid=6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Means there are 'id', 'target', and 'sample_type' columns (3), and all others contain whether 'categorical' (15) or 'continuous' (492) word in their names (3 + 15 + 492 = 510). The split is done before any preprocessing in order to avoid data leakage."
      ],
      "metadata": {
        "id": "kM_ZHBx1TFnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify columns with more than 30% NaN values in dev set\n",
        "nan_threshold_perc = 30\n",
        "nan_percentages_dev = df_dev.isna().mean() * 100\n",
        "high_nan_cols = nan_percentages_dev[nan_percentages_dev > nan_threshold_perc]\n",
        "\n",
        "print(f\"Number of columns with more than {nan_threshold_perc}% NaN in DEV set: {len(high_nan_cols)}\")\n",
        "if len(high_nan_cols) > 0:\n",
        "    print(f\"Columns to be dropped: {list(high_nan_cols.index[:10])}...\" if len(high_nan_cols) > 10 else f\"Columns to be dropped: {list(high_nan_cols.index)}\")\n",
        "\n",
        "# Drop high NaN columns from all splits\n",
        "df_dev.drop(columns=high_nan_cols.index, inplace=True)\n",
        "df_test.drop(columns=high_nan_cols.index, inplace=True)\n",
        "df_valid.drop(columns=high_nan_cols.index, inplace=True)\n",
        "\n",
        "print(f\"\\nAfter dropping high NaN columns:\")\n",
        "print(f\"Dev shape: {df_dev.shape}\")\n",
        "print(f\"Test shape: {df_test.shape}\")\n",
        "print(f\"Valid shape: {df_valid.shape}\")\n",
        "\n",
        "# Update column lists after dropping\n",
        "categorical_cols = [col for col in df_dev.columns if 'categorical' in col.lower()]\n",
        "continuous_cols = [col for col in df_dev.columns if 'continuous' in col.lower()]\n",
        "\n",
        "print(f\"Updated: {len(categorical_cols)} categorical columns and {len(continuous_cols)} continuous columns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7P8r4083-qo",
        "outputId": "70558b17-f6f3-4654-daf7-4c01db300c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of columns with more than 30% NaN in DEV set: 178\n",
            "Columns to be dropped: ['continuous_471', 'continuous_472', 'continuous_480', 'continuous_481', 'continuous_482', 'continuous_483', 'continuous_484', 'continuous_485', 'continuous_486', 'continuous_487']...\n",
            "\n",
            "After dropping high NaN columns:\n",
            "Dev shape: (19368, 510)\n",
            "Test shape: (6130, 510)\n",
            "Valid shape: (6000, 510)\n",
            "Updated: 15 categorical columns and 492 continuous columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 178 columns with more than 30% of NaN values were deleted, which is a good practise of preprocessing. The assession was made based only on the dev set, so that now data leakage happens."
      ],
      "metadata": {
        "id": "KJk6lu3Z5IXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_data(df_subset, categorical_cols, continuous_cols):\n",
        "    \"\"\"Function to perform imputation on a given dataframe\"\"\"\n",
        "    dataset_name = df_subset['sample_type'].iloc[0]\n",
        "    print(f\"\\nImputing {dataset_name} set...\")\n",
        "\n",
        "    # For categorical columns - use mode\n",
        "    for col in categorical_cols:\n",
        "        if not df_subset[col].isna().all():\n",
        "            mode_value = df_subset[col].mode()\n",
        "            if len(mode_value) > 0:\n",
        "                df_subset[col] = df_subset[col].fillna(mode_value[0])\n",
        "\n",
        "    # For continuous columns - use median\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    df_subset[continuous_cols] = imputer.fit_transform(df_subset[continuous_cols])\n",
        "\n",
        "    print(f\"NaN count after imputation: {df_subset.isna().sum().sum()}\")\n",
        "    return df_subset\n",
        "\n",
        "# Apply imputation to each split\n",
        "df_dev = impute_data(df_dev, categorical_cols, continuous_cols)\n",
        "df_test = impute_data(df_test, categorical_cols, continuous_cols)\n",
        "df_valid = impute_data(df_valid, categorical_cols, continuous_cols)\n",
        "\n",
        "print(\"\\n=== Final Dataset Summary ===\")\n",
        "print(f\"Dev: {df_dev.shape}, NaN count: {df_dev.isna().sum().sum()}\")\n",
        "print(f\"Test: {df_test.shape}, NaN count: {df_test.isna().sum().sum()}\")\n",
        "print(f\"Valid: {df_valid.shape}, NaN count: {df_valid.isna().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju_fbMvJ2NFr",
        "outputId": "d82b5591-4103-455b-98ea-cdc3d162be1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imputing dev set...\n",
            "NaN count after imputation: 0\n",
            "\n",
            "Imputing test set...\n",
            "NaN count after imputation: 0\n",
            "\n",
            "Imputing valid set...\n",
            "NaN count after imputation: 0\n",
            "\n",
            "=== Final Dataset Summary ===\n",
            "Dev: (19368, 510), NaN count: 0\n",
            "Test: (6130, 510), NaN count: 0\n",
            "Valid: (6000, 510), NaN count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and targets\n",
        "feature_cols = [col for col in df_dev.columns if col not in ['sample_type', 'target', 'id']]\n",
        "X_dev = df_dev[feature_cols]\n",
        "y_dev = df_dev['target']\n",
        "X_test = df_test[feature_cols]\n",
        "y_test = df_test['target']\n",
        "X_valid = df_valid[feature_cols]\n",
        "y_valid = df_valid['target']\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_dev = label_encoder.fit_transform(y_dev)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "y_valid = label_encoder.transform(y_valid)\n",
        "\n",
        "print(\"Feature and target shapes:\")\n",
        "print(f\"Development set: X_dev={X_dev.shape}, y_dev={y_dev.shape}\")\n",
        "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "print(f\"Validation set: X_valid={X_valid.shape}, y_valid={y_valid.shape}\")\n",
        "\n",
        "# Analyze class imbalance\n",
        "print(\"\\n=== CLASS IMBALANCE ANALYSIS ===\")\n",
        "dev_class_counts = pd.Series(y_dev).value_counts().sort_index()\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "for i, count in enumerate(dev_class_counts):\n",
        "    percentage = (count / len(y_dev)) * 100\n",
        "    print(f\"{class_names[i]}: {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "# Calculate scale_pos_weight for XGBoost\n",
        "scale_pos_weight = len(y_dev[y_dev == 0]) / len(y_dev[y_dev == 1])\n",
        "print(f\"Scale pos weight (for XGBoost): {scale_pos_weight:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUoOOONG21UM",
        "outputId": "9442b97e-6113-481e-afb7-7de2b6624dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature and target shapes:\n",
            "Development set: X_dev=(19368, 507), y_dev=(19368,)\n",
            "Test set: X_test=(6130, 507), y_test=(6130,)\n",
            "Validation set: X_valid=(6000, 507), y_valid=(6000,)\n",
            "\n",
            "=== CLASS IMBALANCE ANALYSIS ===\n",
            "bad: 1,528 (7.89%)\n",
            "good: 17,840 (92.11%)\n",
            "Scale pos weight (for XGBoost): 0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ö†Ô∏è WARNING: This cell takes 20+ minutes to run!\n",
        "# Only run this if you want to find new hyperparameters\n",
        "# Otherwise, skip to the next cell which uses pre-found optimal parameters\n",
        "\n",
        "PERFORM_HYPERPARAMETER_SEARCH = False  # Set to True to run the search\n",
        "\n",
        "if PERFORM_HYPERPARAMETER_SEARCH:\n",
        "    print(\"üîç Starting hyperparameter search...\")\n",
        "    print(\"‚è∞ This will take approximately 20-30 minutes...\")\n",
        "\n",
        "    # Define parameter grid\n",
        "    param_dist = {\n",
        "        'n_estimators': [400, 500, 600],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.005, 0.01, 0.05],\n",
        "        'subsample': [0.5, 0.6, 0.7],\n",
        "        'colsample_bytree': [0.6, 0.7, 0.8],\n",
        "        'gamma': [0, 0.1],\n",
        "        'reg_alpha': [0, 0.01, 0.1],\n",
        "        'reg_lambda': [0.1, 1],\n",
        "        'scale_pos_weight': [scale_pos_weight]\n",
        "    }\n",
        "\n",
        "    # Initialize XGBoost and RandomizedSearchCV\n",
        "    xgb = XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0)\n",
        "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=xgb,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=20,\n",
        "        cv=stratified_cv,\n",
        "        scoring='average_precision',\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Perform search\n",
        "    start_time = time.time()\n",
        "    random_search.fit(X_dev, y_dev)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
        "    print(\"\\nBest hyperparameters found:\")\n",
        "    for param, value in random_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "    print(f\"Best CV Average Precision: {random_search.best_score_:.4f}\")\n",
        "\n",
        "    # Store the best model\n",
        "    best_xgb_from_search = random_search.best_estimator_\n",
        "\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipping hyperparameter search - using pre-found optimal parameters\")"
      ],
      "metadata": {
        "id": "s9GLIpr53dWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Use pre-found optimal hyperparameters\n",
        "# These were found through previous RandomizedSearchCV runs\n",
        "\n",
        "OPTIMAL_PARAMS = {\n",
        "    'subsample': 0.5,\n",
        "    'scale_pos_weight': scale_pos_weight,  # Use calculated value\n",
        "    'reg_lambda': 1,\n",
        "    'reg_alpha': 0.01,\n",
        "    'n_estimators': 400,\n",
        "    'max_depth': 5,\n",
        "    'learning_rate': 0.01,\n",
        "    'gamma': 0,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    'eval_metric': 'logloss',\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "print(\"üîß Training XGBoost with optimal parameters...\")\n",
        "print(\"Optimal parameters:\")\n",
        "for param, value in OPTIMAL_PARAMS.items():\n",
        "    if param != 'scale_pos_weight':  # Don't print the long float\n",
        "        print(f\"  {param}: {value}\")\n",
        "print(f\"  scale_pos_weight: {OPTIMAL_PARAMS['scale_pos_weight']:.3f}\")\n",
        "\n",
        "# Train the model with optimal parameters\n",
        "start_time = time.time()\n",
        "best_xgb = XGBClassifier(**OPTIMAL_PARAMS)\n",
        "best_xgb.fit(X_dev, y_dev)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"‚úÖ Model trained in {(end_time - start_time):.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIczYlKp36EU",
        "outputId": "195b4c76-fc9c-4248-8155-d32e2ab0f4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Training XGBoost with optimal parameters...\n",
            "Optimal parameters:\n",
            "  subsample: 0.5\n",
            "  reg_lambda: 1\n",
            "  reg_alpha: 0.01\n",
            "  n_estimators: 400\n",
            "  max_depth: 5\n",
            "  learning_rate: 0.01\n",
            "  gamma: 0\n",
            "  colsample_bytree: 0.8\n",
            "  random_state: 42\n",
            "  eval_metric: logloss\n",
            "  verbosity: 0\n",
            "  scale_pos_weight: 0.086\n",
            "‚úÖ Model trained in 39.89 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "y_pred_dev = best_xgb.predict(X_dev)\n",
        "y_pred_valid = best_xgb.predict(X_valid)\n",
        "\n",
        "# Get prediction probabilities (important for fraud detection)\n",
        "y_prob_dev = best_xgb.predict_proba(X_dev)[:, 1]  # Probability of fraud (class 1)\n",
        "y_prob_valid = best_xgb.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_basic_metrics(y_true, y_pred, dataset_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n=== {dataset_name} SET METRICS ===\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")  # Of predicted frauds, how many were actually fraud?\n",
        "    print(f\"Recall:    {recall:.4f}\")     # Of actual frauds, how many did we catch?\n",
        "    print(f\"F1-Score:  {f1:.4f}\")        # Harmonic mean of precision and recall\n",
        "\n",
        "# Evaluate both sets\n",
        "evaluate_basic_metrics(y_dev, y_pred_dev, \"DEVELOPMENT\")\n",
        "evaluate_basic_metrics(y_valid, y_pred_valid, \"VALIDATION\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1_rYwTw7lMt",
        "outputId": "a736849d-f188-47d1-b0ef-a5d2b429f9e1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DEVELOPMENT SET METRICS ===\n",
            "Accuracy:  0.8214\n",
            "Precision: 0.9770\n",
            "Recall:    0.8254\n",
            "F1-Score:  0.8949\n",
            "\n",
            "=== VALIDATION SET METRICS ===\n",
            "Accuracy:  0.8133\n",
            "Precision: 0.9464\n",
            "Recall:    0.8442\n",
            "F1-Score:  0.8923\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}